{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rospy\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sklearn.metrics as metrics\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from std_msgs.msg import Header\n",
    "from sensor_msgs.msg import Image, PointCloud, CameraInfo, PointCloud2, PointField\n",
    "import ros_numpy\n",
    "import glob\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(f'runs1/ScanObjectNN/tensorboard')\n",
    "\n",
    "test_true = []\n",
    "test_pred = []\n",
    "\n",
    "class H5Dataset(Dataset):\n",
    "    def __init__(self, pnt_cld_array,label_array, num_points):\n",
    "        # pnt_cld_array = np.array([[pnt_cld_array[0]], [pnt_cld_array[0]]])\n",
    "        # label = np.array([[label[0]], [[0]]])\n",
    "        self.data = pnt_cld_array[:].astype('float32')\n",
    "        self.label = label_array[:].astype('int64')\n",
    "        self.num_points = num_points     \n",
    "        # self.data = np.concatenate(self.data, axis=0)\n",
    "        # self.label = np.concatenate(self.label, axis=0)\n",
    "        # print(self.data.shape, '++++++++++++++')\n",
    "    \n",
    "    def __getitem__(self, item): \n",
    "        pointcloud = self.data[item][:self.num_points]\n",
    "        # print(\"this is pointcloud\", pointcloud)\n",
    "        label = self.label[item]\n",
    "        return pointcloud, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "def load_data(partition):\n",
    "    BASE_DIR = os.path.dirname(os.path.abspath(''))\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    # print(DATA_DIR, '_+-')\n",
    "    all_data = []\n",
    "    all_label = []\n",
    "    for h5_name in glob.glob('data/scanobjectnn/%s_objectdataset*.h5'%partition):\n",
    "        print(h5_name)\n",
    "        f = h5py.File(h5_name, 'r')\n",
    "        # print(f)\n",
    "        try:\n",
    "            data = f['data'][:].astype('float32')\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data -= data_mean\n",
    "        furthest_distance = np.max(np.sqrt(np.sum(abs(data)**2,axis=-1)))\n",
    "        data /= furthest_distance\n",
    "        label = f['label'][:].astype('int64')\n",
    "        f.close()\n",
    "        all_data.append(data)\n",
    "        all_label.append(label)\n",
    "\n",
    "    all_data = np.concatenate(all_data, axis=0)\n",
    "    all_label = np.concatenate(all_label, axis=0)\n",
    "    return all_data, all_label\n",
    "\n",
    "\n",
    "def translate_pointcloud(pointcloud):\n",
    "    xyz1 = np.random.uniform(low=2./3., high=3./2., size=[3])\n",
    "    xyz2 = np.random.uniform(low=-0.2, high=0.2, size=[3])\n",
    "       \n",
    "    translated_pointcloud = np.add(np.multiply(pointcloud, xyz1), xyz2).astype('float32')\n",
    "    return translated_pointcloud\n",
    "\n",
    "\n",
    "def jitter_pointcloud(pointcloud, sigma=0.01, clip=0.02):\n",
    "    N, C = pointcloud.shape\n",
    "    pointcloud += np.clip(sigma * np.random.randn(N, C), -1*clip, clip)\n",
    "    return pointcloud\n",
    "\n",
    "\n",
    "class ModelNet40(Dataset):\n",
    "    def __init__(self, num_points, partition='training'):\n",
    "        self.data, self.label = load_data(partition)\n",
    "        print(self.label)\n",
    "        self.num_points = num_points\n",
    "        self.partition = partition        \n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        pointcloud = self.data[item][:self.num_points]\n",
    "        label = self.label[item]\n",
    "        if self.partition == 'train':\n",
    "            pointcloud = translate_pointcloud(pointcloud)\n",
    "            np.random.shuffle(pointcloud)\n",
    "        return pointcloud, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "\n",
    "def cal_loss(pred, gold, smoothing=True):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2, 1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
    " \n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n",
    "    return idx\n",
    "\n",
    "def get_graph_feature(x, k=20, idx=None):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    if idx is None:\n",
    "        idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "\n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    " \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :]\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "  \n",
    "    return feature\n",
    "\n",
    "\n",
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, output_channels=15):\n",
    "        super(DGCNN, self).__init__()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, 512, kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.linear1 = nn.Linear(512*2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=0.5)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=0.5)\n",
    "        self.linear3 = nn.Linear(256, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = get_graph_feature(x, k=20)\n",
    "        x = self.conv1(x)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x1, k=20)\n",
    "        x = self.conv2(x)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x2, k=20)\n",
    "        x = self.conv3(x)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x3, k=20)\n",
    "        x = self.conv4(x)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def train():\n",
    "    train_loader = DataLoader(ModelNet40(partition='training', num_points=512), num_workers=8,batch_size=40, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(ModelNet40(partition='test', num_points=512), num_workers=8,batch_size=40, shuffle=True, drop_last=False)\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    #Try to load models\n",
    "    model = DGCNN().to(device)\n",
    "    \n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "\n",
    "    if True:\n",
    "        print(\"Use SGD\")\n",
    "        opt = optim.SGD(model.parameters(), lr=0.001*100, momentum=0.0, weight_decay=1e-4)\n",
    "    else:\n",
    "        print(\"Use Adam\")\n",
    "        opt = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    scheduler = CosineAnnealingLR(opt, 20, eta_min=0.001)\n",
    "    \n",
    "    criterion = cal_loss\n",
    "\n",
    "    best_test_acc = 0\n",
    "    step = 0\n",
    "    for epoch in range(20):\n",
    "        scheduler.step()\n",
    "        # Train\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        for data, label in train_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data = data.permute(0, 2, 1)\n",
    "            batch_size = data.size()[0]\n",
    "            opt.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_true.append(label.cpu().numpy())\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        outstr = 'Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch,train_loss*1.0/count,metrics.accuracy_score(train_true, train_pred),metrics.balanced_accuracy_score(train_true, train_pred))\n",
    "        writer.add_scalar('Training loss', train_loss*1.0/count, global_step=step)\n",
    "        writer.add_scalar('Training Accuracy', metrics.balanced_accuracy_score(train_true, train_pred), global_step=step)\n",
    "        print(outstr)\n",
    "        # Test\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            data = data.permute(0, 2, 1)\n",
    "            batch_size = data.size()[0]\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, label)\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            test_loss += loss.item() * batch_size\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        outstr = 'Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch,test_loss*1.0/count,test_acc,avg_per_class_acc)\n",
    "        writer.add_scalar('Testing loss', test_loss*1.0/count, global_step=step)\n",
    "        writer.add_scalar('Testing Accuracy', test_acc, global_step=step)\n",
    "        step += 1\n",
    "        print(outstr)\n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'model.t7')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(data, label=[[5]]):\n",
    "    \n",
    "    # pcd = o3d.io.read_point_cloud(\"test1(1).pcd\")\n",
    "    # pnt_cld = np.asarray(pcd.points) \n",
    "    pnt_cld = np.array([data])\n",
    "    print(pnt_cld.shape, '_____________')\n",
    "    # pnt_cld = np.asarray(pnt_cld, dtype=\"float32\")\n",
    "    #pnt_cld = torch.from_numpy(pnt_cld.astype(\"float32\"))\n",
    "    label = np.array(label)\n",
    "    # label = np.asarray(label)\n",
    "    # label = torch.from_numpy(label.astype('long'))\n",
    "    num_points = 512\n",
    "    model_path = \"/home/r2d2/hsr_rss_project/src/hsr_cnn_detectron/src/checkpoints/dgcnn_2048/models/model.t7\"\n",
    "    test_loader = DataLoader(H5Dataset(pnt_cld_array=pnt_cld,label_array=label,num_points=num_points))\n",
    "    # test_loader = DataLoader(ModelNet40(num_points=num_points,partition=\"test\"))\n",
    "    #DataLoader(H5Dataset(pnt_cld_array=pnt_cld,label_array=label,num_points=num_points))\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\")  #\"cuda\" if args.cuda else \n",
    "\n",
    "    #Try to load models\n",
    "    model = DGCNN().to(device)\n",
    "    # model = nn.DataParallel(model)\n",
    "    model.load_state_dict(torch.load(model_path),strict = False)\n",
    "    model = model.eval()\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    global test_true\n",
    "    global test_pred\n",
    "    # label_check = [[6]]\n",
    "    # label_check = torch.tensor(label_check)\n",
    "    for data, labels in test_loader:\n",
    "        # print(\"this is labels\", labels)\n",
    "        data, labels = data.to(device), labels.to(device).squeeze()\n",
    "        print(data.shape)\n",
    "        # print(\"this is data\",data.dtype)\n",
    "        data = data.permute(0, 2, 1)\n",
    "        batch_size = data.size()[0]\n",
    "        logits = model(data)\n",
    "        print(logits)\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        #print(preds)\n",
    "        test_true.append(labels.cpu().numpy())\n",
    "        test_pred.append([int(i) for i in preds.detach().cpu().numpy()])\n",
    "    print(\"this is tru\",test_true)\n",
    "    print(\"this is tes\",test_pred)\n",
    "    # test_true = np.concatenate(test_true)\n",
    "    # test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    outstr = 'Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc)\n",
    "    print(outstr)\n",
    "\n",
    "\n",
    "class hsr_dgcnn(object):\n",
    "    '''\n",
    "    @To-DO\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.rgb_image = None\n",
    "        self.depth_image = None\n",
    "        self.pcd = None\n",
    "        self.rgbd = None\n",
    "        self.bridge = CvBridge()\n",
    "        self.loop_rate = rospy.Rate(0.25)\n",
    "        self.sub = rospy.Subscriber('/segmented_point_ros', PointCloud2, callback=self.dgcnn)\n",
    "        self.pointclouds = None\n",
    "\n",
    "\n",
    "    def dgcnn(self, msg):\n",
    "        rospy.loginfo('Message Received')\n",
    "        self.pointclouds = ros_numpy.point_cloud2.pointcloud2_to_xyz_array(msg)\n",
    "        self.pointclouds -= np.mean(self.pointclouds, axis=0)\n",
    "        # print(self.pointclouds)\n",
    "        test(self.pointclouds)\n",
    "\n",
    "    \n",
    "    def start(self):\n",
    "        rospy.loginfo('[+] hsr_cnn_detection_node fired!')\n",
    "        rospy.spin()\n",
    "        self.bridge = CvBridge()\n",
    "        while not rospy.is_shutdown():\n",
    "            self.rate.sleep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/scanobjectnn/training_objectdataset_augmented25rot.h5\n",
      "data/scanobjectnn/training_objectdataset.h5\n",
      "data/scanobjectnn/training_objectdataset_augmentedrot.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     train()\n",
      "Cell \u001b[0;32mIn[29], line 233\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m():\n\u001b[0;32m--> 233\u001b[0m     train_loader \u001b[39m=\u001b[39m DataLoader(ModelNet40(partition\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining\u001b[39;49m\u001b[39m'\u001b[39;49m, num_points\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m), num_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,batch_size\u001b[39m=\u001b[39m\u001b[39m35\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    234\u001b[0m     test_loader \u001b[39m=\u001b[39m DataLoader(ModelNet40(partition\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, num_points\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m), num_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m,batch_size\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    236\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 95\u001b[0m, in \u001b[0;36mModelNet40.__init__\u001b[0;34m(self, num_points, partition)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, num_points, partition\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m load_data(partition)\n\u001b[1;32m     96\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel)\n\u001b[1;32m     97\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_points \u001b[39m=\u001b[39m num_points\n",
      "Cell \u001b[0;32mIn[29], line 67\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(partition)\u001b[0m\n\u001b[1;32m     65\u001b[0m data_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(data, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     66\u001b[0m data \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m data_mean\n\u001b[0;32m---> 67\u001b[0m furthest_distance \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(np\u001b[39m.\u001b[39msqrt(np\u001b[39m.\u001b[39;49msum(\u001b[39mabs\u001b[39;49m(data)\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m2\u001b[39;49m,axis\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n\u001b[1;32m     68\u001b[0m data \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m furthest_distance\n\u001b[1;32m     69\u001b[0m label \u001b[39m=\u001b[39m f[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m][:]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2259\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   2257\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> 2259\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   2260\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/scanobjectnn/test_objectdataset_augmented25_norot.h5',\n",
       " 'data/scanobjectnn/test_objectdataset_augmentedrot_scale75.h5',\n",
       " 'data/scanobjectnn/test_objectdataset_augmentedrot.h5',\n",
       " 'data/scanobjectnn/test_objectdataset.h5',\n",
       " 'data/scanobjectnn/test_objectdataset_augmented25rot.h5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "glob.glob('data/scanobjectnn/test_objectdataset*.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
